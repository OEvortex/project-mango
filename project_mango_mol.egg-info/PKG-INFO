Metadata-Version: 2.4
Name: project-mango-mol
Version: 0.1.0
Summary: Modular Layer (MoL) system for fusing transformer blocks from different LLMs
Author: Project Mango Team
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: torch>=2.0.0
Requires-Dist: transformers>=4.30.0
Requires-Dist: accelerate>=0.20.0
Requires-Dist: numpy>=1.21.0
Requires-Dist: scipy>=1.7.0
Requires-Dist: einops>=0.6.0
Requires-Dist: datasets>=2.12.0
Requires-Dist: safetensors>=0.3.0
Requires-Dist: huggingface_hub>=0.15.0
Requires-Dist: hf_xet
Requires-Dist: PyYAML>=6.0
Requires-Dist: pyyaml-include>=1.3.0
Requires-Dist: wandb>=0.15.0
Requires-Dist: tensorboard>=2.8.0
Requires-Dist: psutil>=5.8.0
Requires-Dist: tqdm>=4.64.0
Requires-Dist: pytest>=7.0.0
Requires-Dist: pytest-cov>=4.0.0
Requires-Dist: black>=22.0.0
Requires-Dist: flake8>=5.0.0
Requires-Dist: mypy>=0.991
Requires-Dist: types-PyYAML>=6.0.0
Dynamic: author
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: license-file
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# Project Mango - Enhanced MoL System ü•≠

**Modular Layer (MoL) System for LLMs with Advanced Model Merging**

A powerful toolkit that combines:
- üîÑ **Dynamic runtime fusion** of transformer layers from different LLMs  
- üîÄ **MergeKit-style model merging** with SLERP, TIES, Task Arithmetic, and Linear methods
- ‚ö° **Memory-efficient operations** with lazy loading and smart device placement
- üõ†Ô∏è **YAML configuration system** for easy merge specification
- üñ•Ô∏è **CLI interface** similar to mergekit-yaml

## üöÄ Quick Start

### Model Merging (MergeKit-style)

**Using CLI:**
```bash
# Generate example configurations
mol-merge examples ./configs

# Validate configuration
mol-merge validate config.yml

# Perform merge
mol-merge config.yml ./merged_model --device cuda --verbose
```

**Using Python API:**
```python
from mol import SlerpMerge
from mol.merge_methods.base_merge import MergeConfig

# SLERP merge
config = MergeConfig(
    method="slerp",
    models=["gpt2", "distilgpt2"],
    parameters={"t": 0.5},
    output_path="./merged_model"
)

slerp = SlerpMerge(config)
models = slerp.load_models()
merged = slerp.merge(models)
slerp.save_merged_model(merged)
```

## Architecture

The MoL system consists of:

1. **Block Extractors**: Extract transformer blocks from different model architectures
2. **Adapters**: Bridge dimensional differences between models
3. **Routers**: Decide which expert (model layer) to use for each input
4. **MoL Runtime**: Orchestrates the entire fusion process
5. **Training Pipeline**: Fine-tune adapters and routers for optimal performance

## Examples

### Running the Comprehensive Demo
```bash
# Basic inference demonstration
python examples/comprehensive_demo.py --inference-only --small-models

# Full demo with training and evaluation  
python examples/comprehensive_demo.py --train --eval --small-models

# Large model demonstration
python examples/comprehensive_demo.py --train --eval
```

### Basic Layer Fusion
```python
from mol import MoLRuntime
from mol.core.mol_runtime import MoLConfig

# Create configuration
config = MoLConfig(
    models=["microsoft/DialoGPT-small", "distilgpt2"],
    adapter_type="linear",
    router_type="simple",
    max_layers=4
)

# Initialize MoL runtime
mol = MoLRuntime(config)

# Setup embeddings and LM head
mol.setup_embeddings()
mol.setup_lm_head()

# Add fusion layers
mol.add_layer([
    ("microsoft/DialoGPT-small", 0),
    ("distilgpt2", 0)
], layer_idx=0)

# Forward pass with dynamic routing
inputs = mol.tokenizer("Hello, how are you?", return_tensors="pt")
hidden_states, router_stats = mol.forward(
    inputs['input_ids'], 
    inputs['attention_mask'],
    return_router_stats=True
)
```

### Pushing to Hugging Face Hub üåê

**Install HuggingFace Hub (optional):**
```bash
pip install huggingface_hub
```

**Push MoL models:**
```python
# Option 1: Push lightweight MoL runtime (~50-100MB)
mol.push_to_hf(
    repo_id="your-username/mol-model",
    fusion_type="runtime",
    commit_message="Upload MoL runtime"
)

# Option 2: Push fully fused static model (~2GB)
mol.push_to_hf(
    repo_id="your-username/fused-model", 
    fusion_type="fused",
    fusion_method="weighted_average",
    commit_message="Upload fused model"
)
```

**CLI Usage:**
```bash
# Push MoL runtime
mol-merge push-hf your-username/mol-model \
  --mol-checkpoint ./model.pt \
  --fusion-type runtime

# Push fully fused model
mol-merge push-hf your-username/fused-model \
  --mol-checkpoint ./model.pt \
  --fusion-type fused \
  --fusion-method weighted_average
```

### SafeTensors Support üîí

**Secure model serialization (recommended):**
```python
# SafeTensors enabled by default
mol.save_checkpoint("./model", use_safetensors=True)
loaded_mol = MoLRuntime.load_checkpoint("./model")

# Training with SafeTensors
training_config = TrainingConfig(
    use_safetensors=True,  # Default: True
    output_dir="./checkpoints"
)

# Manual SafeTensors operations
from mol import save_model_safe, load_model_safe
save_model_safe(model, "./model", metadata={"version": "1.0"})
metadata = load_model_safe(model, "./model")
```

**Benefits:**
- üõ°Ô∏è **Security**: No arbitrary code execution (unlike pickle)
- ‚ö° **Performance**: Faster loading and memory mapping
- üîç **Transparency**: Inspectable file format
- ‚úÖ **Integrity**: Built-in validation and checksums

### üèóÔ∏è Universal Architecture Support

MoL now supports **ALL transformer architectures** available in HuggingFace Transformers (120+) with dynamic detection and secure loading:

### üîç Automatic Architecture Detection

- **Dynamic Discovery**: Automatically detects any transformer architecture
- **Component Mapping**: Finds layers, embeddings, and LM heads automatically
- **Architecture Classification**: Groups models into families (decoder-only, encoder-only, etc.)
- **No Manual Updates**: Handles new architectures without code changes

### üõ°Ô∏è Security First

```python
# Secure by default - no remote code execution
config = MoLConfig(
    models=["gpt2", "bert-base-uncased", "t5-small"],
    trust_remote_code=False  # Default: secure
)

# Only enable for trusted models
config_trusted = MoLConfig(
    models=["custom-model-requiring-remote-code"],
    trust_remote_code=True  # Explicit opt-in with warnings
)
```

### üè∑Ô∏è Supported Architecture Families

| Family | Examples | MoL Support |
|--------|----------|-------------|
| **Decoder-Only** | GPT-2, GPT-Neo, Llama, Mistral, Falcon, OPT, BLOOM, Qwen, Yi | ‚úÖ Full Support |
| **Encoder-Only** | BERT, RoBERTa, DistilBERT, ELECTRA, DeBERTa, ALBERT | ‚úÖ Full Support |
| **Encoder-Decoder** | T5, BART, Pegasus, Marian, UL2, FLAN-T5 | ‚úÖ Full Support |
| **Vision** | ViT, DeiT, Swin, BEiT, ConvNeXt | ‚úÖ Full Support |
| **Multimodal** | CLIP, FLAVA, LayoutLM, LXMERT | ‚úÖ Full Support |

### üí° Usage Examples

```python
from mol import MoLRuntime, MoLConfig

# Mix different architecture families
config = MoLConfig(
    models=[
        "gpt2",                    # Decoder-only
        "bert-base-uncased",       # Encoder-only  
        "t5-small",               # Encoder-decoder
    ],
    adapter_type="linear",
    router_type="simple"
)

mol_runtime = MoLRuntime(config)
mol_runtime.setup_embeddings()
mol_runtime.setup_lm_head()

# Add fusion layers from different architectures
mol_runtime.add_layer([
    ("gpt2", 0),
    ("bert-base-uncased", 0),
    ("t5-small", 0)
], layer_idx=0)
```

### üîß Architecture Handler

```python
from mol.core import UniversalArchitectureHandler

# Create handler (secure by default)
handler = UniversalArchitectureHandler(trust_remote_code=False)

# Detect any architecture
arch_info = handler.detect_architecture("microsoft/DialoGPT-medium")
print(f"Architecture: {arch_info.architecture_type}")
print(f"Family: {arch_info.architecture_family}")
print(f"Layers: {arch_info.num_layers}")
print(f"Hidden Dim: {arch_info.hidden_dim}")
print(f"Layer Path: {arch_info.layer_path}")
print(f"Supports Causal LM: {arch_info.supports_causal_lm}")
```

### ‚ö° Performance Features

- **Caching**: Architecture detection results are cached
- **Lazy Loading**: Models loaded only when needed
- **Memory Efficient**: Smart device placement and memory management
- **Fast Introspection**: Quick component discovery without full model loading

### üîí Security Configuration

```bash
# CLI with security options
mol-merge config.yml ./output --trust-remote-code  # Explicit opt-in
mol-merge config.yml ./output                      # Secure by default

# Configuration file
merge_method: slerp
models:
  - gpt2
  - bert-base-uncased
trust_remote_code: false  # Default: secure
output_path: ./merged_model
```

## Training Adapters and Routers
```python
from mol.training.trainer import MoLTrainer, TrainingConfig

# Training configuration
train_config = TrainingConfig(
    learning_rate=1e-4,
    router_learning_rate=1e-5,
    batch_size=8,
    max_epochs=5,
    freeze_experts=True
)

# Initialize trainer
trainer = MoLTrainer(mol, train_config)

# Train the system
trainer.train(train_dataloader, eval_dataloader)
```

### Individual Component Usage
```python
# Using adapters directly
from mol.core.adapters import LinearAdapter, BottleneckAdapter

linear_adapter = LinearAdapter(512, 768, init_identity=True)
bottleneck_adapter = BottleneckAdapter(512, 768, bottleneck_dim=128)

# Using routers directly  
from mol.core.routers import SimpleRouter, TokenLevelRouter

simple_router = SimpleRouter(768, num_experts=3, pooling_type="mean")
token_router = TokenLevelRouter(768, num_experts=3, top_k=2)
```

## Installation

```bash
pip install -r requirements.txt
pip install -e .
```

## Development Status

This project follows a phased development approach:

- ‚úÖ **Phase 1**: Design & Infrastructure (Complete)
- ‚úÖ **Phase 2**: Prototype Runtime (Complete)
- ‚úÖ **Phase 3**: Small-scale Training & Stabilization (Complete)
- üöß **Phase 4**: Fine-tuning & Specialization (In Progress)
- ‚è≥ **Phase 5**: Evaluation & Ablation (Planned)
- ‚è≥ **Phase 6**: Deployment & Optimization (Planned)

### Current Capabilities

‚úÖ **Core Components**:
- Linear and Bottleneck adapters with identity initialization
- SimpleRouter (pooled) and TokenLevelRouter (per-token) with top-k routing
- Support for GPT-2, GPT-Neo, GPT-J, LLaMA, BERT, RoBERTa, DistilBERT architectures
- Memory-efficient lazy loading and offloading
- Comprehensive training pipeline with adapter/router optimization

‚úÖ **Training Features**:
- Identity initialization for warm start
- Separate learning rates for adapters and routers  
- Router entropy and load balancing regularization
- Gradient checkpointing and memory optimization
- Wandb integration for experiment tracking

‚úÖ **Testing & Examples**:
- Unit tests for all core components
- Basic fusion demo with real models
- Complete training example with small models

## License

MIT License - see LICENSE file for details.
